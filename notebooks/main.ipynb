{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Ordinary Differential Equations\n",
    "A discussion of an interesting new paradigm in Neural Networks, adapted from \"Neural Ordinary Differential Equations\" by Chen et al. 2019 (https://arxiv.org/pdf/1806.07366.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Neural Network Setup\n",
    "Neural networks are a set of algorithms that are loosely based on the functonality of the human brain. These algorithms are designed to learn patterns in numerical data, by propagating input data through a series of hidden layers, with each layer being composed of several artificial 'neurons' or nodes. The final result is the output layer, a numerical representation of our desired prediction.\n",
    "\n",
    "Consider an input dataset composed of a set of $m$ features: $(\\vec{x_1}, \\vec{x_2}, ..., \\vec{x_m})$ . <br>\n",
    "We add a bias component to this dataset, $\\vec{x_0} = 1$, a vector of ones, yielding our input layer, $\\vec{x} := \\vec{x_0}, ... \\vec{x_m}$. <br>\n",
    "We need to now consider how to pass this input to our first hidden layer. Suppose this the first layer is composed of $n$ nodes, we define a set of weights $w=\\vec{w_1},\\vec{w_2} ..., \\vec{w_n}$ where each $\\vec{w_i}$ is a vector of length $m$ corresponding to node $i$. We then define $\\vec{z_i} := \\vec{x}\\cdot \\vec{w_{i}}$. <br>\n",
    "\n",
    "We are passing $\\vec{z_i}$ to each node in the first hidden layer, but but what occurs at the node, and what is output at the next layer? The answer is the activation function. For the purpose of this discussion, we will focus solely on the sigmoid activation function $\\sigma(z) = \\dfrac{1}{1+e^{-z}}$, though it should be noted that many others exist. At each node, we will compute $\\sigma(z_i)$ and that's it! We have our single layer representation. <br>\n",
    "\n",
    "To extrapolate to a full neural network, we simply repeat this process, using the output of the activation functions of the previous layer as the new input layer, taking the dot product with a new set of weights and computing the activation of these new inputs. The final layer will be our output layer, $\\vec{y}$, and will have a number of neurons corresponding to our desired predictable (if we are predicting hand-written digits as in the exemplary MNIST dataset, our $\\vec{y}$ will be a vector of length 10). <br>\n",
    "\n",
    "Our problem is thus clear, we must compute: $W$, the weights, $L$, the number of layers and $n_{}$ the number of neurons in each layer. And we must do so efficiently, as there will often be many many weights to compute and many layers in our network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Computing parameters and hyperparameters\n",
    "We will seperate the values we wish to compute into two categories: 'parameters' (the weights) and the 'hyperparameters' (the number of layers and the number of neurons in each layer). In general, determining the hyperparameters is much more of an art than a science, and requires significant experimentation and intuition. On the other hand, determining the parameters can be done by following one of several well-defined protocols. <br>\n",
    "For our purposes, we will simply define a cost-function, which will turn out to be an aggregate of the cost function used in typical logistic regression (__NOTE: if you don't know logistic regression, stop here and go learn that first__). For now, I will simply state the cost function of a neural network implementing the sigmoid activation function: <br> \n",
    "\\begin{align}\n",
    "J(\\Theta) = - \\dfrac{1}{m}\\sum_{i=1}^m \\sum_{k=1}^K\\left[\n",
    "y_{k}^{(i)} \\log((h_{\\Theta}(x^{(i)}))_k) + (1-y)\\log(1-(h_{\\Theta}(x^{(i)}))_k) \n",
    "\\right]\n",
    "+ \\dfrac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}\\left(\n",
    "\\Theta_{j,i}^{l}\n",
    "\\right)^{2}\n",
    "\\end{align}\n",
    "where $L$ is the total number of layers in the network, $s_l$ is the number of nodes in layer $l$, $K$ is the number of output classes/units and $\\lambda$ is a regularization parameter, to prevent overfitting (another hyperparameter to compute). \n",
    "\n",
    "With this cost function, we will choose starting values for the parameters, feed these forward through the network, and calculate the loss, known as the feedforward step. We then take this loss to update the weight and bias values moving backward through the network, known as the backpropagation step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Neural Networks in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 - Definitions \n",
    "As per [this](https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6) tutorial, we will define the input layer $\\textbf{x}$, an arbitrary amount of hidden layers, an output layer $\\hat{\\textbf{y}}$, the set of weights $\\textbf{W}$ and biases $\\textbf{b}$ and the activation function $\\sigma$.\n",
    "\n",
    "We will now create this class in Python, for a neural network with one hidden layer, having 4 nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
    "        self.weights2   = np.random.rand(4,1)                 \n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this class definition, we will add a feedforward step, where we assume the bias values $\\textbf{b}$ are $0$ for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Training\n",
    "To train this network, we need to find appropriate values for $\\textbf{W}$ and $\\textbf{b}$. We do so by feeding forward through the network with an initial set of parameters, and subsequently updating these parameters using backpropagation with an appropriate cost function. Adding the feedforward to our class definition is trivial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.input      = x\n",
    "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
    "        self.weights2   = np.random.rand(4,1)                 \n",
    "        self.y          = y\n",
    "        self.output     = np.zeros(self.y.shape)\n",
    "\n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to add the backpropagation step, we must determine the gradient of our cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. - MNIST Dataset\n",
    "We will now work towards building a standard neural network from scratch, in Python, using the MNIST handwritten number dataset (a toy dataset commonly used in teaching image recognition)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nODEs)",
   "language": "python",
   "name": "nodes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
