{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Ordinary Differential Equations\n",
    "A discussion of an interesting new paradigm in Neural Networks, adapted from \"Neural Ordinary Differential Equations\" by Chen et al. 2019 (https://arxiv.org/pdf/1806.07366.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Neural Network Setup\n",
    "Neural networks are a set of algorithms that are loosely based on the functonality of the human brain. These algorithms are designed to learn patterns in numerical data, by propagating input data through a series of hidden layers, with each layer being composed of several artificial 'neurons' or nodes. The final result is the output layer, a numerical representation of our desired prediction.\n",
    "\n",
    "Consider an input dataset composed of a set of $m$ features: $(\\vec{x_1}, \\vec{x_2}, ..., \\vec{x_m})$ . <br>\n",
    "We add a bias component to this dataset, $\\vec{x_0} = 1$, a vector of ones, yielding our input layer, $\\vec{x} := \\vec{x_0}, ... \\vec{x_m}$. <br>\n",
    "We need to now consider how to pass this input to our first hidden layer. Suppose this the first layer is composed of $n$ nodes, we define a set of weights $w=\\vec{w_1},\\vec{w_2} ..., \\vec{w_n}$ where each $\\vec{w_i}$ is a vector of length $m$ corresponding to node $i$. We then define $\\vec{z_i} := \\vec{x}\\cdot \\vec{w_{i}}$. <br>\n",
    "\n",
    "We are passing $\\vec{z_i}$ to each node in the first hidden layer, but but what occurs at the node, and what is output at the next layer? The answer is the activation function. For the purpose of this discussion, we will focus solely on the sigmoid activation function $\\sigma(z) = \\dfrac{1}{1+e^{-z}}$, though it should be noted that many others exist. At each node, we will compute $\\sigma(z_i)$ and that's it! We have our single layer representation. <br>\n",
    "\n",
    "To extrapolate to a full neural network, we simply repeat this process, using the output of the activation functions of the previous layer as the new input layer, taking the dot product with a new set of weights and computing the activation of these new inputs. The final layer will be our output layer, $\\vec{y}$, and will have a number of neurons corresponding to our desired predictable (if we are predicting hand-written digits as in the exemplary MNIST dataset, our $\\vec{y}$ will be a vector of length 10). <br>\n",
    "\n",
    "Our problem is thus clear, we must compute: $W$, the weights, $L$, the number of layers and $n_{}$ the number of neurons in each layer. And we must do so efficiently, as there will often be many many weights to compute and many layers in our network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Computing parameters and hyperparameters\n",
    "We will seperate the values we wish to compute into two categories: 'parameters' (the weights) and the 'hyperparameters' (the number of layers and the number of neurons in each layer). In general, determining the hyperparameters is much more of an art than a science, and requires significant experimentation and intuition. On the other hand, determining the parameters can be done by following one of several well-defined protocols. <br>\n",
    "For our purposes, we will simply define a cost-function, which will turn out to be an aggregate of the cost function used in typical logistic regression (__NOTE: if you don't know logistic regression, stop here and go learn that first__). For now, I will simply state the cost function of a neural network implementing the sigmoid activation function: <br> \n",
    "\\begin{align}\n",
    "J(\\Theta) = - \\dfrac{1}{m}\\sum_{i=1}^m \\sum_{k=1}^K\\left[\n",
    "y_{k}^{(i)} \\log((h_{\\Theta}(x^{(i)}))_k) + (1-y)\\log(1-(h_{\\Theta}(x^{(i)}))_k) \n",
    "\\right]\n",
    "+ \\dfrac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_{l+1}}\\left(\n",
    "\\Theta_{j,i}^{l}\n",
    "\\right)^{2}\n",
    "\\end{align}\n",
    "where $L$ is the total number of layers in the network, $s_l$ is the number of nodes in layer $l$, $K$ is the number of output classes/units and $\\lambda$ is a regularization parameter, to prevent overfitting (another hyperparameter to compute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nODEs)",
   "language": "python",
   "name": "nodes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
